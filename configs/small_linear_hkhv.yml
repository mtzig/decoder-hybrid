model:
  # These feed LLTransformerConfig
  # vocab_size: 32000        # will default to tokenizer.vocab_size if omitted
  dim: 768
  depth: 12
  num_heads: 8
  head_dim: 64 
  hkv_processor_factory: "linear"  # or "linear" / "mamba2"

data:
  dataset_name: "HuggingFaceFW/fineweb-edu"
  split: "train"
  text_column: "text"
  tokenizer_name: "meta-llama/Meta-Llama-3-8B"  # swap to a real LLaMA tokenizer if you have access
  block_size: 1024
  eval_ratio: 0.01
  max_train_samples: 200000   # optional for dev; remove to train full
  streaming: true           # set true if you want to stream then convert to map-style

training:
  output_dir: "./outputs/lltransformer-linear-yolo3-orig"
  seed: 1234
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 16
  learning_rate: 6.0e-4
  weight_decay: 0.01
  num_train_epochs: 1
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  logging_steps: 1
  save_steps: 2000
  eval_strategy: "steps"
  eval_steps: 100
  save_total_limit: 2
  bf16: true
  fp16: false
  dataloader_num_workers: 4
  report_to: ["wandb"]

logging:
  wandb_project: "lltransformer-fineweb-edu"
  wandb_run_name: "init-test-run"

hub:
  push_to_hub: true
  # hub_model_id: "your-username/LLTransformer-fineweb-edu"
  hub_private_repo: false
  hub_strategy: "end"
