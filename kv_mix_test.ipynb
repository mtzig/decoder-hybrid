{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5b2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def lin_kv_block_memory_efficient(\n",
    "    X, W_Q, W_QK, W_KK, W_VK, W_QV, W_KV, W_VV, eps=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-efficient (non-parallelized) Lin-KV Block.\n",
    "    - Single Python loop over t\n",
    "    - O(d^2) extra state via prefix accumulators\n",
    "    - No O(n^2) stacks kept\n",
    "\n",
    "    Args:\n",
    "      X:   (n, d)\n",
    "      W_*: (d, d)\n",
    "\n",
    "    Returns:\n",
    "      Y: (n, d)\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    device, dtype = X.device, X.dtype\n",
    "\n",
    "    # Projections\n",
    "    Q  = X @ W_Q\n",
    "    QK = X @ W_QK\n",
    "    KK = X @ W_KK\n",
    "    VK = X @ W_VK\n",
    "    QV = X @ W_QV\n",
    "    KV = X @ W_KV\n",
    "    VV = X @ W_VV\n",
    "\n",
    "    # Prefix accumulators\n",
    "    S_K = torch.zeros(d, d, device=device, dtype=dtype)\n",
    "    Z_K = torch.zeros(d,     device=device, dtype=dtype)\n",
    "    S_V = torch.zeros(d, d, device=device, dtype=dtype)\n",
    "    Z_V = torch.zeros(d,     device=device, dtype=dtype)\n",
    "\n",
    "    Y = torch.empty(n, d, device=device, dtype=dtype)\n",
    "\n",
    "    for t in range(n):\n",
    "        # Update prefix sums with step t\n",
    "        S_K = S_K + torch.outer(KK[t], VK[t])  # (d,d)\n",
    "        Z_K = Z_K + KK[t]                       # (d,)\n",
    "        S_V = S_V + torch.outer(KV[t], VV[t])  # (d,d)\n",
    "        Z_V = Z_V + KV[t]                       # (d,)\n",
    "\n",
    "        # Build effective K_(t) and V_(t) for prefix 1..t (no storage beyond this step)\n",
    "        QK_prefix = QK[:t+1]                    # (t+1, d)\n",
    "        QV_prefix = QV[:t+1]                    # (t+1, d)\n",
    "\n",
    "        K_num = QK_prefix @ S_K                 # (t+1, d)\n",
    "        V_num = QV_prefix @ S_V                 # (t+1, d)\n",
    "\n",
    "        K_den = (QK_prefix @ Z_K).unsqueeze(-1) + eps  # (t+1,1)\n",
    "        V_den = (QV_prefix @ Z_V).unsqueeze(-1) + eps  # (t+1,1)\n",
    "\n",
    "        K_t = K_num / K_den                     # (t+1, d)\n",
    "        V_t = V_num / V_den                     # (t+1, d)\n",
    "\n",
    "        # Attention for position t over the prefix\n",
    "        logits = (Q[t] @ K_t.T).unsqueeze(0)    # (1, t+1)\n",
    "        attn = torch.softmax(logits, dim=-1)    # (1, t+1)\n",
    "\n",
    "        # Output y_t\n",
    "        Y[t:t+1] = attn @ V_t                   # (1, d)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "# ---------- Vectorized (parallelized) reference for sanity checks ----------\n",
    "def lin_kv_block_vectorized(X, W_Q, W_QK, W_KK, W_VK, W_QV, W_KV, W_VV, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Parallelized / vectorized O(n^2) time & O(n^2) memory Lin-KV Block.\n",
    "    Returns Y only (no quadratic stacks exposed).\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    device, dtype = X.device, X.dtype\n",
    "\n",
    "    # Projections\n",
    "    Q  = X @ W_Q\n",
    "    QK = X @ W_QK\n",
    "    KK = X @ W_KK\n",
    "    VK = X @ W_VK\n",
    "    QV = X @ W_QV\n",
    "    KV = X @ W_KV\n",
    "    VV = X @ W_VV\n",
    "\n",
    "    # Prefix accumulators for all t\n",
    "    outer_K = torch.einsum(\"nd,ne->nde\", KK, VK)   # (n,d,d)\n",
    "    S_pref_K = outer_K.cumsum(dim=0)               # (n,d,d)\n",
    "    Z_pref_K = KK.cumsum(dim=0)                    # (n,d)\n",
    "\n",
    "    outer_V = torch.einsum(\"nd,ne->nde\", KV, VV)   # (n,d,d)\n",
    "    S_pref_V = outer_V.cumsum(dim=0)               # (n,d,d)\n",
    "    Z_pref_V = KV.cumsum(dim=0)                    # (n,d)\n",
    "\n",
    "    # Build all effective K_(t) and V_(t)\n",
    "    K_num_all = torch.einsum(\"id,tdk->tik\", QK, S_pref_K)  # (n,n,d)\n",
    "    K_den_all = torch.einsum(\"id,td->ti\",  QK, Z_pref_K)   # (n,n)\n",
    "    K_all     = K_num_all / (K_den_all[..., None] + eps)   # (n,n,d)\n",
    "\n",
    "    V_num_all = torch.einsum(\"id,tdk->tik\", QV, S_pref_V)  # (n,n,d)\n",
    "    V_den_all = torch.einsum(\"id,td->ti\",  QV, Z_pref_V)   # (n,n)\n",
    "    V_all     = V_num_all / (V_den_all[..., None] + eps)   # (n,n,d)\n",
    "\n",
    "    # Mask to keep i<=t\n",
    "    tri = torch.tril(torch.ones(n, n, device=device, dtype=torch.bool))\n",
    "    logits = torch.einsum(\"td,tid->ti\", Q, K_all)          # (n,n)\n",
    "    logits = logits.masked_fill(~tri, float(\"-inf\"))\n",
    "\n",
    "    A = torch.softmax(logits, dim=1)                       # (n,n)\n",
    "    Y = torch.einsum(\"ti,tid->td\", A, V_all)               # (n,d)\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4d8a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |Y_mem - Y_vec|: 4.291534423828125e-06\n",
      "✔ Outputs match within tolerance.\n"
     ]
    }
   ],
   "source": [
    "# ---------- Quick sanity check ----------\n",
    "# if __name__ == \"__main__\":\n",
    "# torch.manual_seed(0)\n",
    "n, d = 8, 4\n",
    "X  = torch.randn(n, d)\n",
    "Ws = [torch.randn(d, d) for _ in range(7)]\n",
    "\n",
    "Y_mem = lin_kv_block_memory_efficient(X, *Ws)\n",
    "Y_vec = lin_kv_block_vectorized(X, *Ws)\n",
    "\n",
    "max_diff = (Y_mem - Y_vec).abs().max().item()\n",
    "print(\"max |Y_mem - Y_vec|:\", max_diff)\n",
    "\n",
    "# Optional strict check\n",
    "assert torch.allclose(Y_mem, Y_vec, atol=1e-5, rtol=1e-5), \"Mismatch!\"\n",
    "print(\"✔ Outputs match within tolerance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fde04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a662eb5",
   "metadata": {},
   "source": [
    "# multihead version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e698a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinKVBlockMultiHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head, naive parallelized Lin-KV block.\n",
    "    Complexity: O(B * H * T^2 * d_h) time and memory (quadratic in sequence length).\n",
    "\n",
    "    Args:\n",
    "        d_model: embedding dimension (must equal num_heads * head_dim)\n",
    "        num_heads: number of heads (H)\n",
    "        head_dim: per-head dimension (d_h)\n",
    "        eps: small constant for numerical stability in denominators\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, head_dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        assert d_model == num_heads * head_dim, \\\n",
    "            \"d_model must equal num_heads * head_dim for simple concat output.\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.eps = eps\n",
    "\n",
    "        D = d_model\n",
    "        H = num_heads * head_dim  # flattened heads\n",
    "\n",
    "        # 7 projection matrices as learnable parameters (no bias, to match pseudocode)\n",
    "        self.W_Q  = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_QK = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_KK = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_VK = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_QV = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_KV = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_VV = nn.Parameter(torch.empty(D, H))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for p in [self.W_Q, self.W_QK, self.W_KK, self.W_VK,\n",
    "                  self.W_QV, self.W_KV, self.W_VV]:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def _proj(self, X, W):\n",
    "        # X: (B, T, D) @ (D, H) -> (B, T, H) -> (B, T, num_heads, head_dim)\n",
    "        B, T, _ = X.shape\n",
    "        out = X @ W                           # (B, T, H_flat)\n",
    "        out = out.view(B, T, self.num_heads, self.head_dim)\n",
    "        return out\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (B, T, d_model) or (T, d_model)\n",
    "        Returns:\n",
    "            Y: (B, T, d_model) or (T, d_model) matching input rank.\n",
    "        \"\"\"\n",
    "        squeeze_batch = False\n",
    "        if X.dim() == 2:\n",
    "            X = X.unsqueeze(0)\n",
    "            squeeze_batch = True\n",
    "        B, T, D = X.shape\n",
    "        Hh = self.num_heads\n",
    "        Dh = self.head_dim\n",
    "        device = X.device\n",
    "\n",
    "        # 1) Projections -> (B, T, Hh, Dh)\n",
    "        Q  = self._proj(X, self.W_Q)\n",
    "        QK = self._proj(X, self.W_QK)\n",
    "        KK = self._proj(X, self.W_KK)\n",
    "        VK = self._proj(X, self.W_VK)\n",
    "        QV = self._proj(X, self.W_QV)\n",
    "        KV = self._proj(X, self.W_KV)\n",
    "        VV = self._proj(X, self.W_VV)\n",
    "\n",
    "        # 2) Prefix accumulators for all t (vectorized over time)\n",
    "        #    S_t^K = sum_{i<=t} KK_i ⊗ VK_i  ; Z_t^K = sum_{i<=t} KK_i\n",
    "        #    S_t^V = sum_{i<=t} KV_i ⊗ VV_i  ; Z_t^V = sum_{i<=t} KV_i\n",
    "        # Shapes:\n",
    "        #   KK, VK: (B, T, Hh, Dh)\n",
    "        #   outer_K: (B, T, Hh, Dh, Dh)\n",
    "        outer_K = torch.einsum('bthd,bthe->bthde', KK, VK)\n",
    "        S_pref_K = outer_K.cumsum(dim=1)             # (B, T, Hh, Dh, Dh)\n",
    "        Z_pref_K = KK.cumsum(dim=1)                  # (B, T, Hh, Dh)\n",
    "\n",
    "        outer_V = torch.einsum('bthd,bthe->bthde', KV, VV)\n",
    "        S_pref_V = outer_V.cumsum(dim=1)             # (B, T, Hh, Dh, Dh)\n",
    "        Z_pref_V = KV.cumsum(dim=1)                  # (B, T, Hh, Dh)\n",
    "\n",
    "        # 3) Build all effective K_(t) and V_(t) for all prefixes in parallel\n",
    "        # Indices:\n",
    "        #   t: target time, i: prefix index (i <= t)\n",
    "        # K_num[b,t,i,h,:] = QK[b,i,h,:] @ S_pref_K[b,t,h,:,:]\n",
    "        K_num_all = torch.einsum('bihd,bthde->btihe', QK, S_pref_K)  # (B, T, T, Hh, Dh)\n",
    "        K_den_all = torch.einsum('bihd,bthd->btih',  QK, Z_pref_K)   # (B, T, T, Hh)\n",
    "        K_all = K_num_all / (K_den_all[..., None] + self.eps)        # (B, T, T, Hh, Dh)\n",
    "\n",
    "        V_num_all = torch.einsum('bihd,bthde->btihe', QV, S_pref_V)  # (B, T, T, Hh, Dh)\n",
    "        V_den_all = torch.einsum('bihd,bthd->btih',  QV, Z_pref_V)   # (B, T, T, Hh)\n",
    "        V_all = V_num_all / (V_den_all[..., None] + self.eps)        # (B, T, T, Hh, Dh)\n",
    "\n",
    "        # 4) Attention logits and outputs\n",
    "        # logits[b,t,i,h] = Q[b,t,h,:] · K_all[b,t,i,h,:]\n",
    "        logits = torch.einsum('bthd,btihd->btih', Q, K_all)          # (B, T, T, Hh)\n",
    "\n",
    "        # Causal mask (i <= t)\n",
    "        tri = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))\n",
    "        logits = logits.masked_fill(~tri.view(1, T, T, 1), float('-inf'))\n",
    "\n",
    "        A = F.softmax(logits, dim=2)                                 # (B, T, T, Hh)\n",
    "\n",
    "        # y_head[b,t,h,:] = sum_i A[b,t,i,h] * V_all[b,t,i,h,:]\n",
    "        Y_heads = torch.einsum('btih,btihd->bthd', A, V_all)         # (B, T, Hh, Dh)\n",
    "\n",
    "        # Concat heads -> (B, T, d_model)\n",
    "        Y = Y_heads.reshape(B, T, Hh * Dh)\n",
    "\n",
    "        return Y.squeeze(0) if squeeze_batch else Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ad8253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max per-batch |diff|: [2.384185791015625e-07, 1.1920928955078125e-07]\n",
      "✔ Multi-head (H=1) matches single-head reference.\n"
     ]
    }
   ],
   "source": [
    "# Single-head reference from earlier (returns Y only)\n",
    "def lin_kv_block_vectorized_singlehead(X, W_Q, W_QK, W_KK, W_VK, W_QV, W_KV, W_VV, eps=1e-6):\n",
    "    n, d = X.shape\n",
    "    Q  = X @ W_Q\n",
    "    QK = X @ W_QK\n",
    "    KK = X @ W_KK\n",
    "    VK = X @ W_VK\n",
    "    QV = X @ W_QV\n",
    "    KV = X @ W_KV\n",
    "    VV = X @ W_VV\n",
    "\n",
    "    outer_K = torch.einsum(\"nd,ne->nde\", KK, VK)\n",
    "    S_pref_K = outer_K.cumsum(dim=0)\n",
    "    Z_pref_K = KK.cumsum(dim=0)\n",
    "\n",
    "    outer_V = torch.einsum(\"nd,ne->nde\", KV, VV)\n",
    "    S_pref_V = outer_V.cumsum(dim=0)\n",
    "    Z_pref_V = KV.cumsum(dim=0)\n",
    "\n",
    "    K_num_all = torch.einsum(\"id,tdk->tik\", QK, S_pref_K)\n",
    "    K_den_all = torch.einsum(\"id,td->ti\",  QK, Z_pref_K)\n",
    "    K_all     = K_num_all / (K_den_all[..., None] + eps)\n",
    "\n",
    "    V_num_all = torch.einsum(\"id,tdk->tik\", QV, S_pref_V)\n",
    "    V_den_all = torch.einsum(\"id,td->ti\",  QV, Z_pref_V)\n",
    "    V_all     = V_num_all / (V_den_all[..., None] + eps)\n",
    "\n",
    "    tri = torch.tril(torch.ones(n, n, dtype=torch.bool, device=X.device))\n",
    "    logits = torch.einsum(\"td,tid->ti\", Q, K_all).masked_fill(~tri, float(\"-inf\"))\n",
    "    A = torch.softmax(logits, dim=1)\n",
    "    Y = torch.einsum(\"ti,tid->td\", A, V_all)\n",
    "    return Y\n",
    "\n",
    "# --- test ---\n",
    "torch.manual_seed(0)\n",
    "B, T, d = 2, 6, 8\n",
    "X = torch.randn(B, T, d)\n",
    "\n",
    "# Build multi-head module with 1 head of size d\n",
    "m = LinKVBlockMultiHead(d_model=d, num_heads=1, head_dim=d)\n",
    "with torch.no_grad():\n",
    "    # Copy its weights out to feed the single-head function\n",
    "    W_Q, W_QK, W_KK, W_VK, W_QV, W_KV, W_VV = (\n",
    "        m.W_Q, m.W_QK, m.W_KK, m.W_VK, m.W_QV, m.W_KV, m.W_VV\n",
    "    )\n",
    "\n",
    "# Compare per batch item\n",
    "out_m = m(X)                           # (B, T, d)\n",
    "max_diffs = []\n",
    "for b in range(B):\n",
    "    y_ref = lin_kv_block_vectorized_singlehead(\n",
    "        X[b], W_Q, W_QK, W_KK, W_VK, W_QV, W_KV, W_VV\n",
    "    )\n",
    "    max_diffs.append((out_m[b] - y_ref).abs().max().item())\n",
    "\n",
    "print(\"max per-batch |diff|:\", max_diffs)\n",
    "assert all(md < 1e-5 for md in max_diffs)\n",
    "print(\"✔ Multi-head (H=1) matches single-head reference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dd32eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinKVBlockMultiHeadLinearMem(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head Lin-KV with O(B * H * T * d_h) memory using a FlashAttention-style\n",
    "    streaming softmax over prefix tiles (still O(T^2) time).\n",
    "\n",
    "    Args:\n",
    "        d_model: embedding dimension (must equal num_heads * head_dim)\n",
    "        num_heads: number of heads\n",
    "        head_dim: per-head dim\n",
    "        tile_size: prefix tile size\n",
    "        eps: small constant for denominators\n",
    "        accum_dtype: dtype used for softmax accumulators (default float32; set to float64 for tests)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        head_dim: int,\n",
    "        tile_size: int = 64,\n",
    "        eps: float = 1e-6,\n",
    "        accum_dtype: torch.dtype = torch.float32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model == num_heads * head_dim, \"d_model must equal num_heads * head_dim.\"\n",
    "        self.d_model, self.num_heads, self.head_dim = d_model, num_heads, head_dim\n",
    "        self.tile_size, self.eps, self.accum_dtype = tile_size, eps, accum_dtype\n",
    "\n",
    "        Hflat = num_heads * head_dim\n",
    "        self.W_Q  = nn.Parameter(torch.empty(d_model, Hflat))\n",
    "        self.W_QK = nn.Parameter(torch.empty(d_model, Hflat))\n",
    "        self.W_KK = nn.Parameter(torch.empty(d_model, Hflat))\n",
    "        self.W_VK = nn.Parameter(torch.empty(d_model, Hflat))\n",
    "        self.W_QV = nn.Parameter(torch.empty(d_model, Hflat))\n",
    "        self.W_KV = nn.Parameter(torch.empty(d_model, Hflat))\n",
    "        self.W_VV = nn.Parameter(torch.empty(d_model, Hflat))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for p in [self.W_Q, self.W_QK, self.W_KK, self.W_VK, self.W_QV, self.W_KV, self.W_VV]:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def _proj(self, X, W):\n",
    "        # X: (B,T,D) @ (D,Hflat) -> (B,T,H,dh)\n",
    "        B, T, _ = X.shape\n",
    "        out = X @ W\n",
    "        return out.view(B, T, self.num_heads, self.head_dim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _promote_dtype(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # compute in at least fp32 for stability\n",
    "        return X.float() if X.dtype in (torch.float16, torch.bfloat16) else X\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        squeeze = False\n",
    "        if X.dim() == 2:\n",
    "            X = X.unsqueeze(0)\n",
    "            squeeze = True\n",
    "\n",
    "        X = self._promote_dtype(X)\n",
    "        B, T, D = X.shape\n",
    "        H, Dh = self.num_heads, self.head_dim\n",
    "        dev, dtype = X.device, X.dtype\n",
    "        ts = self.tile_size\n",
    "\n",
    "        # 1) projections\n",
    "        Q  = self._proj(X, self.W_Q)\n",
    "        QK = self._proj(X, self.W_QK)\n",
    "        KK = self._proj(X, self.W_KK)\n",
    "        VK = self._proj(X, self.W_VK)\n",
    "        QV = self._proj(X, self.W_QV)\n",
    "        KV = self._proj(X, self.W_KV)\n",
    "        VV = self._proj(X, self.W_VV)\n",
    "\n",
    "        # 2) prefix states\n",
    "        S_K = torch.zeros(B, H, Dh, Dh, device=dev, dtype=dtype)\n",
    "        Z_K = torch.zeros(B, H, Dh,    device=dev, dtype=dtype)\n",
    "        S_V = torch.zeros(B, H, Dh, Dh, device=dev, dtype=dtype)\n",
    "        Z_V = torch.zeros(B, H, Dh,    device=dev, dtype=dtype)\n",
    "\n",
    "        Y = torch.empty(B, T, H, Dh, device=dev, dtype=dtype)\n",
    "\n",
    "        # helpers for (B,M,H,Dh) x (B,H,Dh,Dh)\n",
    "        def bmh_bmm(A, Bmat):\n",
    "            A2 = A.permute(0, 2, 1, 3).contiguous().view(B * H, A.size(1), Dh)\n",
    "            B2 = Bmat.view(B * H, Dh, Dh)\n",
    "            out = torch.bmm(A2, B2)\n",
    "            return out.view(B, H, A.size(1), Dh).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        def bmh_dot(A, Bvec):\n",
    "            return torch.einsum('bmhd,bhd->bmh', A, Bvec)\n",
    "\n",
    "        neg_inf = torch.finfo(self.accum_dtype).min\n",
    "\n",
    "        for t in range(T):\n",
    "            # update prefix with step t\n",
    "            KK_t, VK_t = KK[:, t], VK[:, t]     # (B,H,Dh)\n",
    "            KV_t, VV_t = KV[:, t], VV[:, t]\n",
    "            S_K = S_K + torch.einsum('bhd,bhe->bhde', KK_t, VK_t)\n",
    "            Z_K = Z_K + KK_t\n",
    "            S_V = S_V + torch.einsum('bhd,bhe->bhde', KV_t, VV_t)\n",
    "            Z_V = Z_V + KV_t\n",
    "\n",
    "            q_t = Q[:, t]  # (B,H,Dh)\n",
    "\n",
    "            # online softmax accumulators (use accum_dtype for stability)\n",
    "            m = torch.full((B, H, 1), neg_inf, device=dev, dtype=self.accum_dtype)\n",
    "            l = torch.zeros((B, H, 1), device=dev, dtype=self.accum_dtype)\n",
    "            y = torch.zeros((B, H, Dh), device=dev, dtype=self.accum_dtype)\n",
    "\n",
    "            i_end = t + 1\n",
    "            for i0 in range(0, i_end, ts):\n",
    "                i1 = min(i0 + ts, i_end)\n",
    "                M = i1 - i0\n",
    "\n",
    "                QK_blk = QK[:, i0:i1]  # (B,M,H,Dh)\n",
    "                QV_blk = QV[:, i0:i1]\n",
    "\n",
    "                # effective K_(t)[i] and V_(t)[i]\n",
    "                K_num = bmh_bmm(QK_blk, S_K)                       # (B,M,H,Dh)\n",
    "                K_den = bmh_dot(QK_blk, Z_K)[..., None] + self.eps # (B,M,H,1)\n",
    "                K_blk = K_num / K_den\n",
    "\n",
    "                V_num = bmh_bmm(QV_blk, S_V)\n",
    "                V_den = bmh_dot(QV_blk, Z_V)[..., None] + self.eps\n",
    "                V_blk = V_num / V_den\n",
    "\n",
    "                # logits in accum dtype\n",
    "                logits_blk = torch.einsum('bhd,bmhd->bhm', q_t, K_blk).to(self.accum_dtype)\n",
    "\n",
    "                # --- canonical merge: compute block with its own max ---\n",
    "                m_blk = logits_blk.max(dim=2, keepdim=True).values                 # (B,H,1)\n",
    "                # exp(logits - m_blk)\n",
    "                w_blk = torch.exp(logits_blk - m_blk)                              # (B,H,M)\n",
    "                # l_blk, y_blk under m_blk\n",
    "                l_blk = w_blk.sum(dim=2, keepdim=True)                             # (B,H,1)\n",
    "                y_blk = torch.einsum('bhm,bmhd->bhd', w_blk, V_blk.to(self.accum_dtype))\n",
    "\n",
    "                # merge with running stats\n",
    "                m_new = torch.maximum(m, m_blk)                                    # (B,H,1)\n",
    "                alpha = torch.exp(m - m_new)                                       # (B,H,1)\n",
    "                beta  = torch.exp(m_blk - m_new)                                   # (B,H,1)\n",
    "\n",
    "                l = l * alpha + l_blk * beta                                       # (B,H,1)\n",
    "                y = y * alpha + y_blk * beta                                       # (B,H,Dh)\n",
    "                m = m_new\n",
    "\n",
    "            Y[:, t] = (y / l).to(dtype)  # back to compute dtype\n",
    "\n",
    "        return (Y.reshape(B, T, H * Dh)).squeeze(0) if squeeze else Y.reshape(B, T, H * Dh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c801aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |naive - linear-mem|: 1.6328125\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m diff = (Y_naive - Y_flash).abs().max().item()\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmax |naive - linear-mem|:\u001b[39m\u001b[33m\"\u001b[39m, diff)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.allclose(Y_naive, Y_flash, atol=\u001b[32m1e-6\u001b[39m, rtol=\u001b[32m1e-6\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✔ match within tolerance\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Naive quadratic-memory version from earlier (unchanged)\n",
    "# class LinKVBlockMultiHeadNaive(...): ...\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "B, T, H, Dh = 2, 24, 3, 8\n",
    "D = H * Dh\n",
    "X = torch.randn(B, T, D)\n",
    "\n",
    "naive = LinKVBlockMultiHeadNaive(d_model=D, num_heads=H, head_dim=Dh)\n",
    "flash = LinKVBlockMultiHeadLinearMem(d_model=D, num_heads=H, head_dim=Dh,\n",
    "                                     tile_size=7, accum_dtype=torch.float64)\n",
    "\n",
    "# Share weights\n",
    "with torch.no_grad():\n",
    "    for n_param, f_param in [\n",
    "        (naive.W_Q, flash.W_Q), (naive.W_QK, flash.W_QK),\n",
    "        (naive.W_KK, flash.W_KK), (naive.W_VK, flash.W_VK),\n",
    "        (naive.W_QV, flash.W_QV), (naive.W_KV, flash.W_KV),\n",
    "        (naive.W_VV, flash.W_VV),\n",
    "    ]:\n",
    "        f_param.copy_(n_param)\n",
    "\n",
    "Y_naive = naive(X)\n",
    "Y_flash = flash(X)\n",
    "\n",
    "diff = (Y_naive - Y_flash).abs().max().item()\n",
    "print(\"max |naive - linear-mem|:\", diff)\n",
    "assert torch.allclose(Y_naive, Y_flash, atol=1e-6, rtol=1e-6)\n",
    "print(\"✔ match within tolerance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fe368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16af7139",
   "metadata": {},
   "source": [
    "# more testtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15134ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinKVBlockMultiHeadFlash(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head Lin-KV block using FlashAttention-style streaming softmax.\n",
    "    Complexity: O(B * H * T^2 * d_h) time, O(B * (T * H * d_h + H * d_h^2)) memory (linear in T).\n",
    "    Args:\n",
    "        d_model: embedding dimension (must equal num_heads * head_dim)\n",
    "        num_heads: number of heads (H)\n",
    "        head_dim: per-head dimension (d_h)\n",
    "        block_size: prefix block size for streaming scan over i<=t\n",
    "        eps: small constant for numerical stability\n",
    "        accum_dtype: dtype to use for prefix accumulators (None => use input dtype)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, head_dim: int,\n",
    "                 block_size: int = 128, eps: float = 1e-6, accum_dtype=None):\n",
    "        super().__init__()\n",
    "        assert d_model == num_heads * head_dim, \\\n",
    "            \"d_model must equal num_heads * head_dim for simple concat output.\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.block_size = block_size\n",
    "        self.eps = eps\n",
    "        self.accum_dtype = accum_dtype\n",
    "\n",
    "        D = d_model\n",
    "        H = num_heads * head_dim  # flattened heads\n",
    "\n",
    "        # 7 projection matrices (no bias, to match your pseudocode)\n",
    "        self.W_Q  = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_QK = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_KK = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_VK = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_QV = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_KV = nn.Parameter(torch.empty(D, H))\n",
    "        self.W_VV = nn.Parameter(torch.empty(D, H))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for p in [self.W_Q, self.W_QK, self.W_KK, self.W_VK,\n",
    "                  self.W_QV, self.W_KV, self.W_VV]:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def load_from(self, other_module):\n",
    "        \"\"\"Convenience: copy weights from your naive implementation.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name in [\"W_Q\", \"W_QK\", \"W_KK\", \"W_VK\", \"W_QV\", \"W_KV\", \"W_VV\"]:\n",
    "                getattr(self, name).copy_(getattr(other_module, name))\n",
    "        return self\n",
    "\n",
    "    def _proj(self, X, W):\n",
    "        # X: (B, T, D) @ (D, H_flat) -> (B, T, H_flat) -> (B, T, Hh, Dh)\n",
    "        B, T, _ = X.shape\n",
    "        Hh, Dh = self.num_heads, self.head_dim\n",
    "        out = X @ W                    # (B, T, H_flat)\n",
    "        out = out.view(B, T, Hh, Dh)   # (B, T, Hh, Dh)\n",
    "        return out\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (B, T, d_model) or (T, d_model)\n",
    "        Returns: (B, T, d_model) or (T, d_model) matching input rank.\n",
    "        \"\"\"\n",
    "        squeeze_batch = False\n",
    "        if X.dim() == 2:\n",
    "            X = X.unsqueeze(0)\n",
    "            squeeze_batch = True\n",
    "\n",
    "        B, T, D = X.shape\n",
    "        Hh, Dh = self.num_heads, self.head_dim\n",
    "        device = X.device\n",
    "        dtype = X.dtype\n",
    "        adtype = self.accum_dtype or dtype  # accumulators dtype\n",
    "\n",
    "        # ---- 1) Projections (linear memory in T) ----\n",
    "        Q  = self._proj(X, self.W_Q)   # (B, T, Hh, Dh)\n",
    "        QK = self._proj(X, self.W_QK)\n",
    "        KK = self._proj(X, self.W_KK)\n",
    "        VK = self._proj(X, self.W_VK)\n",
    "        QV = self._proj(X, self.W_QV)\n",
    "        KV = self._proj(X, self.W_KV)\n",
    "        VV = self._proj(X, self.W_VV)\n",
    "\n",
    "        # ---- 2) Prefix accumulators (maintained online) ----\n",
    "        # S^K, S^V: (B, Hh, Dh, Dh); Z^K, Z^V: (B, Hh, Dh)\n",
    "        S_K = torch.zeros(B, Hh, Dh, Dh, device=device, dtype=adtype)\n",
    "        Z_K = torch.zeros(B, Hh, Dh,     device=device, dtype=adtype)\n",
    "        S_V = torch.zeros(B, Hh, Dh, Dh, device=device, dtype=adtype)\n",
    "        Z_V = torch.zeros(B, Hh, Dh,     device=device, dtype=adtype)\n",
    "\n",
    "        # Output buffer\n",
    "        Y_heads = torch.empty(B, T, Hh, Dh, device=device, dtype=dtype)\n",
    "\n",
    "        eps = torch.as_tensor(self.eps, device=device, dtype=adtype)\n",
    "        tiny = torch.finfo(adtype).tiny\n",
    "\n",
    "        # ---- 3) Sweep over time t, stream over prefix i in blocks ----\n",
    "        for t in range(T):\n",
    "            # Update prefix states with token t (per head)\n",
    "            KK_t = KK[:, t].to(adtype)   # (B, Hh, Dh)\n",
    "            VK_t = VK[:, t].to(adtype)\n",
    "            KV_t = KV[:, t].to(adtype)\n",
    "            VV_t = VV[:, t].to(adtype)\n",
    "\n",
    "            # S_t^K += KK_t^T @ VK_t ; S_t^V += KV_t^T @ VV_t\n",
    "            S_K = S_K + torch.einsum('bhd,bhe->bhde', KK_t, VK_t)\n",
    "            Z_K = Z_K + KK_t\n",
    "            S_V = S_V + torch.einsum('bhd,bhe->bhde', KV_t, VV_t)\n",
    "            Z_V = Z_V + KV_t\n",
    "\n",
    "            # Current query (for logits) at time t\n",
    "            Q_t = Q[:, t]   # (B, Hh, Dh)\n",
    "\n",
    "            # Streaming softmax accumulators per (B,Hh)\n",
    "            # m: running max logits; l: sum of exp; y: weighted sum of v_i^(t)\n",
    "            m = torch.full((B, Hh), -float('inf'), device=device, dtype=adtype)\n",
    "            l = torch.zeros((B, Hh), device=device, dtype=adtype)\n",
    "            y = torch.zeros((B, Hh, Dh), device=device, dtype=adtype)\n",
    "\n",
    "            # Iterate prefix blocks i in [0..t]\n",
    "            for i0 in range(0, t + 1, self.block_size):\n",
    "                i1 = min(i0 + self.block_size, t + 1)\n",
    "                Bi = i1 - i0\n",
    "\n",
    "                QK_blk = QK[:, i0:i1].to(adtype)  # (B, Bi, Hh, Dh)\n",
    "                QV_blk = QV[:, i0:i1].to(adtype)  # (B, Bi, Hh, Dh)\n",
    "\n",
    "                # Numerators and denominators (per head)\n",
    "                # u_i = QK_i @ S_K ; alpha_i = QK_i · Z_K\n",
    "                U_blk = torch.einsum('bihd,bhde->bihe', QK_blk, S_K)            # (B, Bi, Hh, Dh)\n",
    "                alpha_blk = torch.einsum('bihd,bhd->bih', QK_blk, Z_K) + eps    # (B, Bi, Hh)\n",
    "\n",
    "                # s_i = (Q_t · u_i) / alpha_i\n",
    "                s_blk = torch.einsum('bihe,bhe->bih', U_blk, Q_t.to(adtype)) / alpha_blk  # (B,Bi,Hh)\n",
    "\n",
    "                # v_i^(t) = (QV_i @ S_V) / (QV_i · Z_V)\n",
    "                W_blk = torch.einsum('bihd,bhde->bihe', QV_blk, S_V)            # (B, Bi, Hh, Dh)\n",
    "                beta_blk = torch.einsum('bihd,bhd->bih', QV_blk, Z_V) + eps     # (B, Bi, Hh)\n",
    "                V_blk = W_blk / beta_blk[..., None]                             # (B, Bi, Hh, Dh)\n",
    "\n",
    "                # Flash-style streaming softmax update across the block\n",
    "                s_blk_max = s_blk.max(dim=1).values   # (B, Hh)\n",
    "                m_next = torch.maximum(m, s_blk_max)\n",
    "                exp_scale = torch.exp((m - m_next).clamp_min(torch.log(tiny)))  # prevent underflow\n",
    "\n",
    "                # sum_i exp(s_i - m_next)\n",
    "                exp_scores = torch.exp(s_blk - m_next.unsqueeze(1))\n",
    "                l = exp_scale * l + exp_scores.sum(dim=1)\n",
    "\n",
    "                # y = exp_scale * y + sum_i exp(s_i - m_next) * v_i\n",
    "                y = exp_scale.unsqueeze(-1) * y + torch.einsum('bih,bihe->bhe', exp_scores, V_blk)\n",
    "\n",
    "                m = m_next\n",
    "\n",
    "            # Normalize to get output for time t\n",
    "            Y_heads[:, t] = (y / (l.unsqueeze(-1) + tiny)).to(dtype)\n",
    "\n",
    "        # Concat heads -> (B, T, d_model)\n",
    "        Y = Y_heads.reshape(B, T, Hh * Dh)\n",
    "        return Y.squeeze(0) if squeeze_batch else Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "275d6954",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "log(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m max_err < \u001b[32m2e-4\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m rel_err < \u001b[32m2e-3\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFlash and naive outputs differ too much!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# A couple of sizes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43msanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHh\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDh\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m sanity_check(seed=\u001b[32m1\u001b[39m, B=\u001b[32m2\u001b[39m, T=\u001b[32m32\u001b[39m, Hh=\u001b[32m4\u001b[39m, Dh=\u001b[32m16\u001b[39m)\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# Try on GPU/bfloat16 or float16 if you'd like:\u001b[39;00m\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# sanity_check(seed=2, B=2, T=64, Hh=8, Dh=16, dtype=torch.bfloat16)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36msanity_check\u001b[39m\u001b[34m(seed, B, T, Hh, Dh, device, dtype)\u001b[39m\n\u001b[32m     17\u001b[39m flash.load_from(naive)  \u001b[38;5;66;03m# ensure identical weights\u001b[39;00m\n\u001b[32m     19\u001b[39m Y_naive = naive(X)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m Y_flash = \u001b[43mflash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m max_err = (Y_naive - Y_flash).abs().max().item()\n\u001b[32m     23\u001b[39m rel_err = ( (Y_naive - Y_flash).abs() / (Y_naive.abs().clamp_min(\u001b[32m1e-5\u001b[39m)) ).max().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dai/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dai/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mLinKVBlockMultiHeadFlash.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    147\u001b[39m s_blk_max = s_blk.max(dim=\u001b[32m1\u001b[39m).values   \u001b[38;5;66;03m# (B, Hh)\u001b[39;00m\n\u001b[32m    148\u001b[39m m_next = torch.maximum(m, s_blk_max)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m exp_scale = torch.exp((m - m_next).clamp_min(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiny\u001b[49m\u001b[43m)\u001b[49m))  \u001b[38;5;66;03m# prevent underflow\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# sum_i exp(s_i - m_next)\u001b[39;00m\n\u001b[32m    152\u001b[39m exp_scores = torch.exp(s_blk - m_next.unsqueeze(\u001b[32m1\u001b[39m))\n",
      "\u001b[31mTypeError\u001b[39m: log(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "# ---- Sanity check script ----\n",
    "import torch\n",
    "\n",
    "# Reuse your naive implementation\n",
    "# (Assuming your LinKVBlockMultiHead class is already defined in scope)\n",
    "\n",
    "def sanity_check(seed=0, B=2, T=32, Hh=4, Dh=16, device=None, dtype=torch.float32):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    D = Hh * Dh\n",
    "    X = torch.randn(B, T, D, device=device, dtype=dtype)\n",
    "\n",
    "    naive = LinKVBlockMultiHead(d_model=D, num_heads=Hh, head_dim=Dh).to(device, dtype)\n",
    "    flash = LinKVBlockMultiHeadFlash(d_model=D, num_heads=Hh, head_dim=Dh,\n",
    "                                     block_size=8, eps=1e-6).to(device, dtype)\n",
    "    flash.load_from(naive)  # ensure identical weights\n",
    "\n",
    "    Y_naive = naive(X)\n",
    "    Y_flash = flash(X)\n",
    "\n",
    "    max_err = (Y_naive - Y_flash).abs().max().item()\n",
    "    rel_err = ( (Y_naive - Y_flash).abs() / (Y_naive.abs().clamp_min(1e-5)) ).max().item()\n",
    "\n",
    "    print(f\"device={device} dtype={dtype} B={B} T={T} H={Hh} Dh={Dh}\")\n",
    "    print(f\"max abs error: {max_err:.3e}, max rel error: {rel_err:.3e}\")\n",
    "\n",
    "    # Be reasonably strict; adjust tol if using fp16/bfloat16\n",
    "    assert max_err < 2e-4 or rel_err < 2e-3, \"Flash and naive outputs differ too much!\"\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # A couple of sizes\n",
    "sanity_check(seed=0, B=1, T=16, Hh=2, Dh=8)\n",
    "sanity_check(seed=1, B=2, T=32, Hh=4, Dh=16)\n",
    "    # Try on GPU/bfloat16 or float16 if you'd like:\n",
    "    # sanity_check(seed=2, B=2, T=64, Hh=8, Dh=16, dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae904701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
